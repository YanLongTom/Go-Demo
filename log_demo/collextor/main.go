package main

import (
	"bufio"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"os/exec"
	"path/filepath"
	"regexp"
	"strings"
	"time"

	"github.com/IBM/sarama"
	"github.com/fsnotify/fsnotify"
)

// FileCollectorLogEntry æ–‡ä»¶é‡‡é›†å™¨æ—¥å¿—æ¡ç›®
type FileCollectorLogEntry struct {
	Timestamp    string            `json:"timestamp"`
	Level        string            `json:"level"`
	Service      string            `json:"service"`
	Message      string            `json:"message"`
	FileName     string            `json:"file_name"`
	FilePath     string            `json:"file_path"`
	LineNumber   int               `json:"line_number"`
	RequestID    string            `json:"request_id"`
	OriginalLine string            `json:"original_line"`
	Metadata     map[string]string `json:"metadata"`
	CollectedAt  string            `json:"collected_at"`

	// æ¸…æ´—åçš„æ•°æ®
	CleanedMessage  string            `json:"cleaned_message"`
	ExtractedFields map[string]string `json:"extracted_fields"`
	ProcessedAt     string            `json:"processed_at"`
	DataQuality     string            `json:"data_quality"`
}

// PythonDataCleaner çœŸå®çš„Pythonæ•°æ®æ¸…æ´—å™¨
type PythonDataCleaner struct {
	pythonScript string
}

// NewPythonDataCleaner åˆ›å»ºæ•°æ®æ¸…æ´—å™¨
func NewPythonDataCleaner() *PythonDataCleaner {
	// è·å–å½“å‰ç›®å½•ä¸‹çš„Pythonè„šæœ¬è·¯å¾„
	scriptPath := "./data_cleaner.py"
	return &PythonDataCleaner{
		pythonScript: scriptPath,
	}
}

// CleanData è°ƒç”¨Pythonè„šæœ¬è¿›è¡Œæ•°æ®æ¸…æ´—
func (cleaner *PythonDataCleaner) CleanData(logEntry *FileCollectorLogEntry) {
	log.Printf("ğŸ [è°ƒç”¨Python] å¤„ç†æ—¥å¿—: %s", logEntry.RequestID)

	// å°†Goç»“æ„ä½“è½¬æ¢ä¸ºJSON
	jsonData, err := json.Marshal(logEntry)
	if err != nil {
		log.Printf("âŒ JSONåºåˆ—åŒ–å¤±è´¥: %v", err)
		return
	}

	// è°ƒç”¨Pythonè„šæœ¬
	result, err := cleaner.callPythonScript(string(jsonData))
	if err != nil {
		log.Printf("âŒ Pythonè„šæœ¬è°ƒç”¨å¤±è´¥: %v", err)
		// è®¾ç½®é»˜è®¤å€¼ï¼Œé˜²æ­¢å¤„ç†å¤±è´¥
		logEntry.CleanedMessage = logEntry.Message
		logEntry.ExtractedFields = make(map[string]string)
		logEntry.ProcessedAt = time.Now().Format(time.RFC3339)
		logEntry.DataQuality = "LOW"
		return
	}

	// è§£æPythonè¿”å›çš„ç»“æœ
	var cleanedEntry FileCollectorLogEntry
	if err := json.Unmarshal([]byte(result), &cleanedEntry); err != nil {
		log.Printf("âŒ Pythonç»“æœè§£æå¤±è´¥: %v", err)
		return
	}

	// æ›´æ–°åŸå§‹æ—¥å¿—æ¡ç›®
	logEntry.CleanedMessage = cleanedEntry.CleanedMessage
	logEntry.ExtractedFields = cleanedEntry.ExtractedFields
	logEntry.ProcessedAt = cleanedEntry.ProcessedAt
	logEntry.DataQuality = cleanedEntry.DataQuality

	log.Printf("âœ… [Pythonå¤„ç†] å®Œæˆ: %s -> %s [è´¨é‡:%s]",
		logEntry.Message[:min(30, len(logEntry.Message))],
		logEntry.CleanedMessage[:min(30, len(logEntry.CleanedMessage))],
		logEntry.DataQuality)
}

// callPythonScript è°ƒç”¨Pythonè„šæœ¬
func (cleaner *PythonDataCleaner) callPythonScript(jsonData string) (string, error) {
	// æ„å»ºPythonå‘½ä»¤
	cmd := exec.Command("python3", cleaner.pythonScript, jsonData)

	// å¦‚æœpython3ä¸å­˜åœ¨ï¼Œå°è¯•python
	if _, err := exec.LookPath("python3"); err != nil {
		cmd = exec.Command("python", cleaner.pythonScript, jsonData)
	}

	// æ‰§è¡Œå‘½ä»¤å¹¶è·å–è¾“å‡º
	output, err := cmd.Output()
	if err != nil {
		// å¦‚æœå‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œå°è¯•è·å–é”™è¯¯ä¿¡æ¯
		if exitError, ok := err.(*exec.ExitError); ok {
			return "", fmt.Errorf("Pythonè„šæœ¬æ‰§è¡Œå¤±è´¥: %s, é”™è¯¯è¾“å‡º: %s", err, string(exitError.Stderr))
		}
		return "", fmt.Errorf("Pythonè„šæœ¬æ‰§è¡Œå¤±è´¥: %v", err)
	}

	return strings.TrimSpace(string(output)), nil
}

// FileCollectorKafkaProducer æ–‡ä»¶é‡‡é›†å™¨ä¸“ç”¨çš„Kafkaç”Ÿäº§è€…
type FileCollectorKafkaProducer struct {
	producer sarama.SyncProducer
}

// NewFileCollectorKafkaProducer åˆ›å»ºæ–‡ä»¶é‡‡é›†å™¨Kafkaç”Ÿäº§è€…
func NewFileCollectorKafkaProducer(brokers []string) (*FileCollectorKafkaProducer, error) {
	config := sarama.NewConfig()
	config.Producer.Return.Successes = true
	config.Producer.RequiredAcks = sarama.WaitForAll // ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
	config.Producer.Retry.Max = 5                    // é‡è¯•æ¬¡æ•°
	config.Producer.Partitioner = sarama.NewHashPartitioner
	config.Producer.Idempotent = true // å¹‚ç­‰æ€§ï¼Œé˜²æ­¢é‡å¤å‘é€
	config.Net.MaxOpenRequests = 1    // ğŸ”‘ å¹‚ç­‰æ€§ç”Ÿäº§è€…å¿…éœ€é…ç½®

	producer, err := sarama.NewSyncProducer(brokers, config)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºKafkaç”Ÿäº§è€…å¤±è´¥: %v", err)
	}

	return &FileCollectorKafkaProducer{producer: producer}, nil
}

// SendMessage å‘é€æ¶ˆæ¯åˆ°Kafka
func (kp *FileCollectorKafkaProducer) SendMessage(topic string, key string, value []byte) error {
	msg := &sarama.ProducerMessage{
		Topic: topic,
		Key:   sarama.StringEncoder(key),
		Value: sarama.ByteEncoder(value),
	}

	partition, offset, err := kp.producer.SendMessage(msg)
	if err != nil {
		return fmt.Errorf("å‘é€æ¶ˆæ¯å¤±è´¥: %v", err)
	}

	log.Printf("ğŸ“¤ æ¶ˆæ¯å‘é€æˆåŠŸ: topic=%s, partition=%d, offset=%d", topic, partition, offset)
	return nil
}

// Close å…³é—­ç”Ÿäº§è€…
func (kp *FileCollectorKafkaProducer) Close() error {
	return kp.producer.Close()
}

// FileCollector æ–‡ä»¶é‡‡é›†å™¨
type FileCollector struct {
	kafka         *FileCollectorKafkaProducer
	watcher       *fsnotify.Watcher
	logDir        string
	topic         string
	filePositions map[string]int64
	dataCleaner   *PythonDataCleaner
}

// LogParser æ—¥å¿—è§£æå™¨
type LogParser struct {
	logPattern *regexp.Regexp
	serviceMap map[string]string
}

// NewLogParser åˆ›å»ºæ—¥å¿—è§£æå™¨
func NewLogParser() *LogParser {
	// åŒ¹é…æ—¥å¿—æ ¼å¼: [timestamp] [level] [request_id] message
	pattern := regexp.MustCompile(`\[([^\]]+)\]\s*\[([^\]]+)\]\s*\[([^\]]+)\]\s*(.+)`)

	serviceMap := map[string]string{
		"nginx.log":       "nginx",
		"application.log": "application",
		"database.log":    "database",
		"security.log":    "security",
	}

	return &LogParser{
		logPattern: pattern,
		serviceMap: serviceMap,
	}
}

// ParseLogLine è§£ææ—¥å¿—è¡Œ
func (p *LogParser) ParseLogLine(line, fileName string, lineNumber int) (*FileCollectorLogEntry, error) {
	matches := p.logPattern.FindStringSubmatch(line)
	if len(matches) != 5 {
		// å¦‚æœä¸åŒ¹é…æ ‡å‡†æ ¼å¼ï¼Œä½œä¸ºæ™®é€šæ¶ˆæ¯å¤„ç†
		return &FileCollectorLogEntry{
			Timestamp:    time.Now().Format(time.RFC3339),
			Level:        "INFO",
			Service:      p.getServiceFromFileName(fileName),
			Message:      strings.TrimSpace(line),
			FileName:     fileName,
			LineNumber:   lineNumber,
			RequestID:    fmt.Sprintf("file_%d_%d", time.Now().Unix(), lineNumber),
			OriginalLine: line,
			CollectedAt:  time.Now().Format(time.RFC3339),
			Metadata:     make(map[string]string),
		}, nil
	}

	// è§£ææ—¶é—´æˆ³
	timestamp := matches[1]
	if parsedTime, err := time.Parse("2006-01-02 15:04:05", timestamp); err == nil {
		timestamp = parsedTime.Format(time.RFC3339)
	}

	// æå–å…ƒæ•°æ®
	metadata := p.extractMetadata(matches[4])

	return &FileCollectorLogEntry{
		Timestamp:    timestamp,
		Level:        matches[2],
		Service:      p.getServiceFromFileName(fileName),
		Message:      matches[4],
		FileName:     fileName,
		LineNumber:   lineNumber,
		RequestID:    matches[3],
		OriginalLine: line,
		CollectedAt:  time.Now().Format(time.RFC3339),
		Metadata:     metadata,
	}, nil
}

// getServiceFromFileName ä»æ–‡ä»¶åè·å–æœåŠ¡å
func (p *LogParser) getServiceFromFileName(fileName string) string {
	if service, exists := p.serviceMap[fileName]; exists {
		return service
	}
	return "unknown"
}

// extractMetadata æå–å…ƒæ•°æ®
func (p *LogParser) extractMetadata(message string) map[string]string {
	metadata := make(map[string]string)

	// æå–é”®å€¼å¯¹ (key=value æ ¼å¼)
	kvPattern := regexp.MustCompile(`(\w+)=([^\s]+)`)
	matches := kvPattern.FindAllStringSubmatch(message, -1)

	for _, match := range matches {
		if len(match) == 3 {
			metadata[match[1]] = match[2]
		}
	}

	return metadata
}

// NewFileCollector åˆ›å»ºæ–‡ä»¶é‡‡é›†å™¨
func NewFileCollector(logDir, topic string, kafka *FileCollectorKafkaProducer) (*FileCollector, error) {
	watcher, err := fsnotify.NewWatcher()
	if err != nil {
		return nil, err
	}

	return &FileCollector{
		kafka:         kafka,
		watcher:       watcher,
		logDir:        logDir,
		topic:         topic,
		filePositions: make(map[string]int64),
		dataCleaner:   NewPythonDataCleaner(),
	}, nil
}

// Start å¯åŠ¨æ–‡ä»¶é‡‡é›†
func (fc *FileCollector) Start() error {
	parser := NewLogParser()

	// æ·»åŠ ç›®å½•ç›‘æ§
	err := fc.watcher.Add(fc.logDir)
	if err != nil {
		return err
	}

	log.Printf("å¼€å§‹ç›‘æ§ç›®å½•: %s", fc.logDir)

	// é¦–æ¬¡è¯»å–æ‰€æœ‰ç°æœ‰æ–‡ä»¶
	err = fc.readExistingFiles(parser)
	if err != nil {
		log.Printf("è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥: %v", err)
	}

	// ç›‘æ§æ–‡ä»¶å˜åŒ–
	go func() {
		for {
			select {
			case event, ok := <-fc.watcher.Events:
				if !ok {
					return
				}

				if event.Op&fsnotify.Write == fsnotify.Write {
					log.Printf("æ–‡ä»¶å˜åŒ–: %s", event.Name)
					fc.processFileChange(event.Name, parser)
				}

			case err, ok := <-fc.watcher.Errors:
				if !ok {
					return
				}
				log.Printf("ç›‘æ§é”™è¯¯: %v", err)
			}
		}
	}()

	return nil
}

// readExistingFiles è¯»å–ç°æœ‰æ–‡ä»¶
func (fc *FileCollector) readExistingFiles(parser *LogParser) error {
	files, err := os.ReadDir(fc.logDir)
	if err != nil {
		return err
	}

	for _, file := range files {
		if file.IsDir() || !strings.HasSuffix(file.Name(), ".log") {
			continue
		}

		filePath := filepath.Join(fc.logDir, file.Name())
		log.Printf("å¤„ç†ç°æœ‰æ–‡ä»¶: %s", filePath)

		err := fc.processFile(filePath, parser)
		if err != nil {
			log.Printf("å¤„ç†æ–‡ä»¶å¤±è´¥ %s: %v", filePath, err)
		}
	}

	return nil
}

// processFileChange å¤„ç†æ–‡ä»¶å˜åŒ–
func (fc *FileCollector) processFileChange(filePath string, parser *LogParser) {
	if !strings.HasSuffix(filePath, ".log") {
		return
	}

	err := fc.processFile(filePath, parser)
	if err != nil {
		log.Printf("å¤„ç†æ–‡ä»¶å˜åŒ–å¤±è´¥ %s: %v", filePath, err)
	}
}

// processFile å¤„ç†å•ä¸ªæ–‡ä»¶
func (fc *FileCollector) processFile(filePath string, parser *LogParser) error {
	file, err := os.Open(filePath)
	if err != nil {
		return err
	}
	defer file.Close()

	// è·å–æ–‡ä»¶å½“å‰ä½ç½®
	fileName := filepath.Base(filePath)
	lastPosition := fc.filePositions[filePath]

	// å®šä½åˆ°ä¸Šæ¬¡è¯»å–çš„ä½ç½®
	_, err = file.Seek(lastPosition, 0)
	if err != nil {
		return err
	}

	scanner := bufio.NewScanner(file)
	lineNumber := int(lastPosition) // ç®€åŒ–çš„è¡Œå·è®¡ç®—

	for scanner.Scan() {
		line := scanner.Text()
		if strings.TrimSpace(line) == "" {
			continue
		}

		lineNumber++

		// è§£ææ—¥å¿—è¡Œ
		logEntry, err := parser.ParseLogLine(line, fileName, lineNumber)
		if err != nil {
			log.Printf("è§£ææ—¥å¿—è¡Œå¤±è´¥: %v", err)
			continue
		}

		logEntry.FilePath = filePath

		// ğŸ æ•°æ®æ¸…æ´—å¤„ç†
		fc.dataCleaner.CleanData(logEntry)

		// å‘é€åˆ°Kafka
		err = fc.sendToKafka(logEntry)
		if err != nil {
			log.Printf("å‘é€åˆ°Kafkaå¤±è´¥: %v", err)
			continue
		}

		log.Printf("é‡‡é›†æ—¥å¿—: %s [%s] %s [è´¨é‡:%s]", logEntry.Service, logEntry.Level, logEntry.CleanedMessage[:min(40, len(logEntry.CleanedMessage))], logEntry.DataQuality)
	}

	// æ›´æ–°æ–‡ä»¶ä½ç½®
	currentPosition, _ := file.Seek(0, 1)
	fc.filePositions[filePath] = currentPosition

	return scanner.Err()
}

// sendToKafka å‘é€åˆ°Kafka
func (fc *FileCollector) sendToKafka(logEntry *FileCollectorLogEntry) error {
	jsonData, err := json.Marshal(logEntry)
	if err != nil {
		return err
	}

	return fc.kafka.SendMessage(fc.topic, logEntry.RequestID, jsonData)
}

// Close å…³é—­é‡‡é›†å™¨
func (fc *FileCollector) Close() error {
	return fc.watcher.Close()
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

func main() {
	// åˆ›å»ºKafkaç”Ÿäº§è€…
	producer, err := NewFileCollectorKafkaProducer([]string{"localhost:9092"})
	if err != nil {
		log.Fatalf("âŒ åˆ›å»ºKafkaç”Ÿäº§è€…å¤±è´¥: %v", err)
	}
	defer producer.Close()

	// åˆ›å»ºæ–‡ä»¶é‡‡é›†å™¨
	logDir := "/root/A-log/log-generator/logs"
	collector, err := NewFileCollector(logDir, "log-data", producer)
	if err != nil {
		log.Fatalf("âŒ åˆ›å»ºæ–‡ä»¶é‡‡é›†å™¨å¤±è´¥: %v", err)
	}
	defer collector.Close()

	log.Println("========================================")
	log.Println("ğŸ“‚ æ–‡ä»¶æ—¥å¿—é‡‡é›†å™¨å¯åŠ¨")
	log.Println("ğŸ çœŸå®Pythonæ•°æ®æ¸…æ´—è„šæœ¬è°ƒç”¨")
	log.Println("========================================")
	log.Printf("ğŸ“ ç›‘æ§ç›®å½•: %s", logDir)
	log.Printf("ğŸ“¤ Kafkaä¸»é¢˜: log-data")
	log.Printf("ğŸ“‹ æ”¯æŒæ–‡ä»¶: *.log")
	log.Printf("ğŸ Pythonè„šæœ¬: ./data_cleaner.py")
	log.Printf("ğŸ§¹ æ•°æ®æ¸…æ´—: çœŸå®Pythonè°ƒç”¨")
	log.Println("========================================")

	// å¯åŠ¨é‡‡é›†
	err = collector.Start()
	if err != nil {
		log.Fatalf("âŒ å¯åŠ¨é‡‡é›†å¤±è´¥: %v", err)
	}

	log.Println("ğŸš€ æ–‡ä»¶é‡‡é›†å™¨å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")

	// ä¿æŒç¨‹åºè¿è¡Œ
	select {}
}
