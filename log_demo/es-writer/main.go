package main

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"log"
	"strings"
	"sync"
	"time"

	"github.com/IBM/sarama"
	"github.com/elastic/go-elasticsearch/v8"
	"github.com/elastic/go-elasticsearch/v8/esapi"
)

// ESConsumerLogEntry ESæ¶ˆè´¹è€…ä½¿ç”¨çš„æ—¥å¿—æ¡ç›®ç»“æž„
type ESConsumerLogEntry struct {
	Timestamp       string            `json:"@timestamp"`
	Level           string            `json:"level"`
	Service         string            `json:"service"`
	Message         string            `json:"message"`
	FileName        string            `json:"file_name,omitempty"`
	FilePath        string            `json:"file_path,omitempty"`
	LineNumber      int               `json:"line_number,omitempty"`
	RequestID       string            `json:"request_id"`
	OriginalLine    string            `json:"original_line,omitempty"`
	Metadata        map[string]string `json:"metadata,omitempty"`
	CollectedAt     string            `json:"collected_at,omitempty"`
	ProcessedAt     string            `json:"processed_at,omitempty"`
	CleanedMessage  string            `json:"cleaned_message,omitempty"`
	ExtractedFields map[string]string `json:"extracted_fields,omitempty"`
	DataQuality     string            `json:"data_quality,omitempty"`

	// é˜²é‡å¤å­—æ®µ
	MessageHash  string `json:"message_hash"`
	EsDocumentID string `json:"es_document_id"`
	ConsumedAt   string `json:"consumed_at"`
	RetryCount   int    `json:"retry_count"`
}

// DuplicateChecker é‡å¤æ£€æŸ¥å™¨
type DuplicateChecker struct {
	processedMessages map[string]bool
	mutex             sync.RWMutex
	maxSize           int
}

// NewDuplicateChecker åˆ›å»ºé‡å¤æ£€æŸ¥å™¨
func NewDuplicateChecker(maxSize int) *DuplicateChecker {
	return &DuplicateChecker{
		processedMessages: make(map[string]bool),
		maxSize:           maxSize,
	}
}

// IsProcessed æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
func (dc *DuplicateChecker) IsProcessed(messageID string) bool {
	dc.mutex.RLock()
	defer dc.mutex.RUnlock()
	return dc.processedMessages[messageID]
}

// MarkProcessed æ ‡è®°ä¸ºå·²å¤„ç†
func (dc *DuplicateChecker) MarkProcessed(messageID string) {
	dc.mutex.Lock()
	defer dc.mutex.Unlock()

	// å¦‚æžœè¶…è¿‡æœ€å¤§å¤§å°ï¼Œæ¸…ç†ä¸€åŠ
	if len(dc.processedMessages) >= dc.maxSize {
		newMap := make(map[string]bool)
		count := 0
		for k, v := range dc.processedMessages {
			if count < dc.maxSize/2 {
				newMap[k] = v
				count++
			}
		}
		dc.processedMessages = newMap
		log.Printf("ðŸ§¹ é‡å¤æ£€æŸ¥å™¨æ¸…ç†å®Œæˆï¼Œä¿ç•™ %d æ¡è®°å½•", count)
	}

	dc.processedMessages[messageID] = true
}

// ElasticsearchClient Elasticsearchå®¢æˆ·ç«¯
type ElasticsearchClient struct {
	client           *elasticsearch.Client
	duplicateChecker *DuplicateChecker
}

// NewElasticsearchClient åˆ›å»ºElasticsearchå®¢æˆ·ç«¯
func NewElasticsearchClient(addresses []string) (*ElasticsearchClient, error) {
	cfg := elasticsearch.Config{
		Addresses: addresses,
		// ä¼˜åŒ–è¿žæŽ¥é…ç½®
		MaxRetries:    5,
		RetryOnStatus: []int{502, 503, 504, 429},
		// è¿žæŽ¥æ± é…ç½®
		DiscoverNodesOnStart:  true,
		DiscoverNodesInterval: 60 * time.Second,
		// è¿žæŽ¥è¶…æ—¶
		//		DialTimeout:    30 * time.Second,
		//		RequestTimeout: 30 * time.Second,
	}

	client, err := elasticsearch.NewClient(cfg)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºESå®¢æˆ·ç«¯å¤±è´¥: %v", err)
	}

	// æµ‹è¯•è¿žæŽ¥
	res, err := client.Info()
	if err != nil {
		return nil, fmt.Errorf("ESè¿žæŽ¥æµ‹è¯•å¤±è´¥: %v", err)
	}
	defer res.Body.Close()

	if res.IsError() {
		return nil, fmt.Errorf("ESæœåŠ¡ä¸å¯ç”¨: %s", res.String())
	}

	log.Println("âœ… Elasticsearchè¿žæŽ¥æˆåŠŸ")
	return &ElasticsearchClient{
		client:           client,
		duplicateChecker: NewDuplicateChecker(10000), // ç¼“å­˜1ä¸‡æ¡è®°å½•
	}, nil
}

// generateDocumentID ç”Ÿæˆæ–‡æ¡£IDï¼ˆé˜²é‡å¤çš„å…³é”®ï¼‰
func (es *ElasticsearchClient) generateDocumentID(logEntry *ESConsumerLogEntry) string {
	// æ–¹æ¡ˆ1ï¼šä½¿ç”¨RequestIDï¼ˆæŽ¨èï¼‰
	if logEntry.RequestID != "" {
		return logEntry.RequestID
	}

	// æ–¹æ¡ˆ2ï¼šä½¿ç”¨å†…å®¹å“ˆå¸Œ
	content := fmt.Sprintf("%s_%s_%s_%s_%d",
		logEntry.Timestamp, logEntry.Service, logEntry.Level, logEntry.Message, logEntry.LineNumber)
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])[:16] // å–å‰16ä½
}

// IndexDocument ç´¢å¼•æ–‡æ¡£åˆ°Elasticsearch (é‡ç‚¹ä¼˜åŒ–é˜²é‡å¤)
func (es *ElasticsearchClient) IndexDocument(index string, logEntry *ESConsumerLogEntry) error {
	// ç”Ÿæˆå”¯ä¸€æ–‡æ¡£ID
	docID := es.generateDocumentID(logEntry)
	logEntry.EsDocumentID = docID

	// ç”Ÿæˆæ¶ˆæ¯å“ˆå¸Œç”¨äºŽé‡å¤æ£€æŸ¥
	messageContent := fmt.Sprintf("%s_%s_%s", logEntry.RequestID, logEntry.Timestamp, logEntry.Message)
	messageHash := sha256.Sum256([]byte(messageContent))
	logEntry.MessageHash = hex.EncodeToString(messageHash[:])[:16]

	// æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
	if es.duplicateChecker.IsProcessed(logEntry.MessageHash) {
		log.Printf("âš ï¸  é‡å¤æ¶ˆæ¯ï¼Œè·³è¿‡: ID=%s", docID)
		return nil
	}

	// æ·»åŠ æ—¶é—´æˆ³
	logEntry.ConsumedAt = time.Now().Format(time.RFC3339)

	jsonDoc, err := json.Marshal(logEntry)
	if err != nil {
		return fmt.Errorf("JSONåºåˆ—åŒ–å¤±è´¥: %v", err)
	}

	// ðŸ”‘ å…³é”®é…ç½®ï¼šé˜²é‡å¤å†™å…¥
	req := esapi.IndexRequest{
		Index:         index,
		DocumentID:    docID, // ä½¿ç”¨ç¡®å®šæ€§ID
		Body:          strings.NewReader(string(jsonDoc)),
		Refresh:       "wait_for", // ç¡®ä¿ç«‹å³å¯æŸ¥è¯¢
		OpType:        "create",   // ðŸš¨ é‡è¦ï¼šåªåˆ›å»ºï¼Œä¸è¦†ç›–
		Timeout:       time.Second * 30,
		IfSeqNo:       nil,
		IfPrimaryTerm: nil,
	}

	res, err := req.Do(context.Background(), es.client)
	if err != nil {
		return fmt.Errorf("ESç´¢å¼•è¯·æ±‚å¤±è´¥: %v", err)
	}
	defer res.Body.Close()

	if res.IsError() {
		// å¦‚æžœæ˜¯å†²çªé”™è¯¯ï¼ˆæ–‡æ¡£å·²å­˜åœ¨ï¼‰ï¼Œåˆ™è®¤ä¸ºæˆåŠŸ
		if res.StatusCode == 409 {
			log.Printf("ðŸ“ æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡: ID=%s", docID)
			es.duplicateChecker.MarkProcessed(logEntry.MessageHash)
			return nil
		}

		// å…¶ä»–é”™è¯¯ï¼Œè§£æžè¯¦æƒ…
		var errResponse map[string]interface{}
		if err := json.NewDecoder(res.Body).Decode(&errResponse); err == nil {
			return fmt.Errorf("ESç´¢å¼•å¤±è´¥ [%s]: %v", res.Status(), errResponse)
		}
		return fmt.Errorf("ESç´¢å¼•å¤±è´¥: %s", res.String())
	}

	// æ ‡è®°ä¸ºå·²å¤„ç†
	es.duplicateChecker.MarkProcessed(logEntry.MessageHash)

	log.Printf("âœ… æ–‡æ¡£ç´¢å¼•æˆåŠŸ: ID=%s, Hash=%s", docID, logEntry.MessageHash[:8])
	return nil
}

// CreateIndexTemplate åˆ›å»ºç´¢å¼•æ¨¡æ¿ (é‡è¦ï¼šä¼˜åŒ–ESæ€§èƒ½)
func (es *ElasticsearchClient) CreateIndexTemplate() error {
	template := `{
		"index_patterns": ["logs-*"],
		"template": {
			"settings": {
				"number_of_shards": 1,
				"number_of_replicas": 0,
				"refresh_interval": "5s",
				"index": {
					"max_result_window": 50000,
					"mapping": {
						"ignore_malformed": true
					}
				}
			},
			"mappings": {
				"properties": {
					"@timestamp": {
						"type": "date",
						"format": "strict_date_optional_time||epoch_millis"
					},
					"level": {
						"type": "keyword"
					},
					"service": {
						"type": "keyword"
					},
					"message": {
						"type": "text",
						"analyzer": "standard"
					},
					"cleaned_message": {
						"type": "text",
						"analyzer": "standard"
					},
					"request_id": {
						"type": "keyword"
					},
					"file_name": {
						"type": "keyword"
					},
					"line_number": {
						"type": "integer"
					},
					"data_quality": {
						"type": "keyword"
					},
					"message_hash": {
						"type": "keyword"
					},
					"es_document_id": {
						"type": "keyword"
					},
					"retry_count": {
						"type": "integer"
					}
				}
			}
		}
	}`

	req := esapi.IndicesPutIndexTemplateRequest{
		Name: "logs-template",
		Body: strings.NewReader(template),
	}

	res, err := req.Do(context.Background(), es.client)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºç´¢å¼•æ¨¡æ¿å¤±è´¥: %v", err)
	}
	defer res.Body.Close()

	if res.IsError() {
		log.Printf("è­¦å‘Š: åˆ›å»ºç´¢å¼•æ¨¡æ¿å¤±è´¥: %s", res.String())
	} else {
		log.Println("âœ… ç´¢å¼•æ¨¡æ¿åˆ›å»ºæˆåŠŸ")
	}

	return nil
}

// KafkaConsumer Kafkaæ¶ˆè´¹è€…
type KafkaConsumer struct {
	consumer sarama.ConsumerGroup
	topics   []string
	groupID  string
}

// NewKafkaConsumer åˆ›å»ºKafkaæ¶ˆè´¹è€…
func NewKafkaConsumer(brokers []string, groupID string, topics []string) (*KafkaConsumer, error) {
	config := sarama.NewConfig()
	config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRoundRobin
	config.Consumer.Offsets.Initial = sarama.OffsetOldest
	config.Consumer.Group.Session.Timeout = 20 * time.Second
	config.Consumer.Group.Heartbeat.Interval = 6 * time.Second
	config.Consumer.MaxProcessingTime = 10 * time.Second
	config.Consumer.Return.Errors = true

	// ðŸ”‘ å…³é”®é…ç½®ï¼šé˜²æ­¢é‡å¤æ¶ˆè´¹
	config.Consumer.Offsets.AutoCommit.Enable = true
	config.Consumer.Offsets.AutoCommit.Interval = 1 * time.Second
	config.Consumer.Group.Rebalance.Timeout = 60 * time.Second

	consumer, err := sarama.NewConsumerGroup(brokers, groupID, config)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºKafkaæ¶ˆè´¹è€…å¤±è´¥: %v", err)
	}

	log.Printf("âœ… Kafkaæ¶ˆè´¹è€…åˆ›å»ºæˆåŠŸï¼Œæ¶ˆè´¹ç»„: %s", groupID)
	return &KafkaConsumer{
		consumer: consumer,
		topics:   topics,
		groupID:  groupID,
	}, nil
}

// ConsumerGroupHandler æ¶ˆè´¹è€…ç»„å¤„ç†å™¨
type ConsumerGroupHandler struct {
	esClient *ElasticsearchClient
	ready    chan bool
}

// Setup æ¶ˆè´¹è€…ç»„è®¾ç½®
func (h *ConsumerGroupHandler) Setup(sarama.ConsumerGroupSession) error {
	close(h.ready)
	return nil
}

// Cleanup æ¶ˆè´¹è€…ç»„æ¸…ç†
func (h *ConsumerGroupHandler) Cleanup(sarama.ConsumerGroupSession) error {
	return nil
}

// ConsumeClaim æ¶ˆè´¹æ¶ˆæ¯ (é‡ç‚¹ä¼˜åŒ–ï¼šESå†™å…¥é€»è¾‘)
func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	for {
		select {
		case message := <-claim.Messages():
			if message == nil {
				return nil
			}

			log.Printf("ðŸ“¥ æŽ¥æ”¶æ¶ˆæ¯: topic=%s, partition=%d, offset=%d, size=%d bytes",
				message.Topic, message.Partition, message.Offset, len(message.Value))

			// è§£æžæ—¥å¿—æ•°æ®
			var logEntry ESConsumerLogEntry
			if err := json.Unmarshal(message.Value, &logEntry); err != nil {
				log.Printf("âŒ JSONè§£æžå¤±è´¥: %v, åŽŸå§‹æ•°æ®: %s", err, string(message.Value))
				session.MarkMessage(message, "")
				continue
			}

			// ç¡®ä¿RequestIDå­˜åœ¨
			if logEntry.RequestID == "" {
				logEntry.RequestID = fmt.Sprintf("kafka_%s_%d_%d", message.Topic, message.Partition, message.Offset)
			}

			// ðŸ”‘ ç¡®ä¿@timestampå­—æ®µå­˜åœ¨å¹¶æ ¼å¼æ­£ç¡®
			if logEntry.Timestamp == "" {
				logEntry.Timestamp = time.Now().Format(time.RFC3339)
			}

			// ç”Ÿæˆç´¢å¼•åï¼ˆæŒ‰æ—¥æœŸåˆ†ç‰‡ï¼‰
			indexName := fmt.Sprintf("logs-%s", time.Now().Format("2006-01-02"))

			// ðŸ”‘ ESå†™å…¥é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤§3æ¬¡ï¼‰
			maxRetries := 3
			var indexErr error

			for retry := 0; retry < maxRetries; retry++ {
				logEntry.RetryCount = retry
				indexErr = h.esClient.IndexDocument(indexName, &logEntry)

				if indexErr == nil {
					break // æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªçŽ¯
				}

				log.Printf("âš ï¸  ESå†™å…¥å¤±è´¥ (é‡è¯• %d/%d): %v", retry+1, maxRetries, indexErr)

				if retry < maxRetries-1 {
					// é€’å¢žå»¶è¿Ÿé‡è¯•ï¼š1s, 2s, 3s
					time.Sleep(time.Duration(retry+1) * time.Second)
				}
			}

			if indexErr != nil {
				log.Printf("âŒ ESå†™å…¥æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡æ¶ˆæ¯: %v", indexErr)
				// å¤±è´¥çš„æ¶ˆæ¯ä¹Ÿè¦æ ‡è®°ï¼Œé¿å…æ— é™é‡è¯•å¯¼è‡´æ¶ˆè´¹é˜»å¡ž
				session.MarkMessage(message, "")
				continue
			}

			log.Printf("âœ… æ—¥å¿—ç´¢å¼•æˆåŠŸ: Service=%s, Level=%s, Quality=%s",
				logEntry.Service, logEntry.Level, logEntry.DataQuality)

			// ðŸ”‘ é‡è¦ï¼šåªæœ‰æˆåŠŸå†™å…¥ESåŽæ‰æäº¤offset
			session.MarkMessage(message, "")

		case <-session.Context().Done():
			return nil
		}
	}
}

// StartConsuming å¼€å§‹æ¶ˆè´¹
func (kc *KafkaConsumer) StartConsuming(esClient *ElasticsearchClient) error {
	handler := &ConsumerGroupHandler{
		esClient: esClient,
		ready:    make(chan bool),
	}

	ctx := context.Background()
	go func() {
		for {
			if err := kc.consumer.Consume(ctx, kc.topics, handler); err != nil {
				log.Printf("âŒ æ¶ˆè´¹é”™è¯¯: %v", err)
			}
			// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦è¢«å–æ¶ˆ
			if ctx.Err() != nil {
				return
			}
			handler.ready = make(chan bool)
		}
	}()

	<-handler.ready
	log.Println("ðŸš€ Kafkaæ¶ˆè´¹è€…å·²å‡†å¤‡å°±ç»ª")

	// å¤„ç†é”™è¯¯
	go func() {
		for err := range kc.consumer.Errors() {
			log.Printf("âŒ æ¶ˆè´¹è€…é”™è¯¯: %v", err)
		}
	}()

	return nil
}

// Close å…³é—­æ¶ˆè´¹è€…
func (kc *KafkaConsumer) Close() error {
	return kc.consumer.Close()
}

func main() {
	log.Println("========================================")
	log.Println("ðŸš€ å¯åŠ¨Elasticsearchæ¶ˆè´¹è€…")
	log.Println("ðŸ”’ é˜²é‡å¤æ¶ˆè´¹ & é˜²é‡å¤å†™å…¥ä¼˜åŒ–ç‰ˆ")
	log.Println("========================================")

	// åˆ›å»ºElasticsearchå®¢æˆ·ç«¯
	esClient, err := NewElasticsearchClient([]string{"http://localhost:9200"})
	if err != nil {
		log.Fatalf("âŒ åˆ›å»ºElasticsearchå®¢æˆ·ç«¯å¤±è´¥: %v", err)
	}

	// åˆ›å»ºç´¢å¼•æ¨¡æ¿ï¼ˆé‡è¦ï¼šä¼˜åŒ–ESæ€§èƒ½ï¼‰
	err = esClient.CreateIndexTemplate()
	if err != nil {
		log.Printf("âš ï¸  åˆ›å»ºç´¢å¼•æ¨¡æ¿å¤±è´¥: %v", err)
	}

	// åˆ›å»ºKafkaæ¶ˆè´¹è€…
	consumer, err := NewKafkaConsumer(
		[]string{"localhost:9092"},
		"log-consumer-group-v2", // ä½¿ç”¨æ–°çš„æ¶ˆè´¹ç»„
		[]string{"log-data"},
	)
	if err != nil {
		log.Fatalf("âŒ åˆ›å»ºKafkaæ¶ˆè´¹è€…å¤±è´¥: %v", err)
	}
	defer consumer.Close()

	log.Println("ðŸ“Š é…ç½®ä¿¡æ¯:")
	log.Println("  Kafka Brokers: localhost:9092")
	log.Println("  Consumer Group: log-consumer-group-v2")
	log.Println("  Topic: log-data")
	log.Println("  Elasticsearch: http://localhost:9200")
	log.Println("  é˜²é‡å¤ç­–ç•¥: æ–‡æ¡£ID + å†…å®¹å“ˆå¸Œ")
	log.Println("  é‡è¯•æœºåˆ¶: æœ€å¤§3æ¬¡ï¼Œé€’å¢žå»¶è¿Ÿ")
	log.Println("========================================")

	// å¼€å§‹æ¶ˆè´¹
	err = consumer.StartConsuming(esClient)
	if err != nil {
		log.Fatalf("âŒ å¼€å§‹æ¶ˆè´¹å¤±è´¥: %v", err)
	}

	log.Println("ðŸŽ¯ å¼€å§‹æ¶ˆè´¹æ—¥å¿—æ•°æ®ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")

	// ä¿æŒç¨‹åºè¿è¡Œ
	select {}
}
